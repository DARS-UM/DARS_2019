---
title: "Pillar 2 - Topic Modeling"
author: "DARS"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: TRUE
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(cache.path = "Cache/Pillar 2/",
                      eval = FALSE)
```

#Setup
##Libraries
```{r library, message = FALSE, eval = TRUE}
library(tidyverse)
library(tidytext)

library(ggwordcloud) # Word Clouds
library(topicmodels)
library(lemon)
library(ggthemes)
library(rlang) #quote
library(ldatuning) # ideal number of topics in LDA

```

##Loading data
We load a list `d_text` with two tibbles of textual data for catalogues and manuals. That is, for $catalogues and $manuals we have a tibble with the following columns: `Course ID`, `year`, `word_original`, `word`.

We also load the environment `Output/data_pillar_2.RDATA` containing the data sets`d_course` and `d_transcript` that we will need to prepare some imputs for our App.
```{r import data, eval = TRUE}
#load
load("./Output/d_text.RDATA")
load("./Output/data_pillar_2.RDATA") #used for app
```
##Helpers
###Preparation of course_title vector
```{r course titles, eval = TRUE}
#Clean titles
course_titles <-  d_text$catalogues %>%
  select(`Course ID`, `Course Title`, department) %>% 
  distinct

empty_titles <- course_titles %>% filter(`Course Title` == "")
```
####Uniform course labels
We do not have the course codes for all courses, in that case we use the course titles as identifiers, for more pleasant display to the user, we will make course labels uniform, for unknown codes we give the label "unknown" and attatch the title as title. 
```{r attatch titles}
uniform_course_labels <- course_titles %>% mutate(`c_title`= case_when(is.na(`Course Title`) ~ `Course ID`,
                                              T ~ `Course Title`),
                         `Course ID` = case_when(str_length(`Course ID`) == 7 ~ `Course ID`,
                                                 T ~ "unknown"),
                         `Course Title` = c_title
                         ) %>% select(-c_title)
```
###Functions for internal faceting
```{r function reorder_within & co., echo = FALSE, eval = TRUE}

# Function for ordering within facet:
# https://github.com/dgrtwo/drlib/blob/master/R/reorder_within.R

reorder_within <- function(x, by, within, fun = mean, sep = "___", ...) {
  new_x <- paste(x, within, sep = sep)
  stats::reorder(new_x, by, FUN = fun)
}

scale_x_reordered <- function(..., sep = "___") {
  reg <- paste0(sep, ".+$")
  ggplot2::scale_x_discrete(labels = function(x) gsub(reg, "", x), ...)
}

scale_y_reordered <- function(..., sep = "___") {
  reg <- paste0(sep, ".+$")
  ggplot2::scale_y_discrete(labels = function(x) gsub(reg, "", x), ...)
}
```

# TF-IDF
We will perform an initial (visual) exploratory analysis of our textual data using TF- IDF.

## Functions for Generating Barplots and Word Clouds
The function `compute_tf_idf` computes the inverse term document frequency, we apply `compute_tf_idf` on descriptions, overviews and manuals. 
```{r compute_tf_idf, cache = TRUE}
#
##Function
compute_tf_idf <- function(data){
  
  data %>%
    count(
      `Course ID`,
      word
      ) %>%
    bind_tf_idf(
      term = word, 
      document = `Course ID`,
      n = n
      )
}

#
##Application
tf_idf <- map(d_text, .f = compute_tf_idf)

#
##Clean workspace
rm(compute_tf_idf)
```

The function `plot_tf_idf` plots a bar-graph of the most important words for each level of granularity and  their corresponding word clouds. 
```{r plot_tf_idf}
plot_tf_idf <- function(data, n_col = 5, id_plot = NULL){
  
  
  #
  # Barplot
  g <- data %>%
    ggplot(
      aes(
        x = reorder_within(word, tf_idf, facet), 
        y = tf_idf
        )
      ) +
    geom_col(
      show.legend = FALSE
      ) +
    labs(
      x = NULL,
      y = "tf-idf"
      ) +
    scale_x_reordered() +
    facet_wrap(
      facets = ~ facet, 
      scales = "free",
      ncol = n_col
      ) +
    coord_flip()
  
  ggsave(paste(id_plot, "_BP.jpeg", sep = ""), path = "Output/Plots/tf_idf", width = 16, height = 8)
  
  
  #
  # Word Cloud
  g <- data %>%
    group_by(
      facet
      ) %>%
    mutate(
      freq = tf_idf / sum(tf_idf)
      ) %>% # normalization within facet
    ggplot(
      aes(
        size = freq ^ 0.7, 
        label = word, 
        color = freq ^ 0.7
        )
      ) +
    geom_text_wordcloud_area(
      area_corr_power = 1,
      eccentricity    = 1,
      rm_outside      = T
      ) +
    scale_radius(
      range = c(2, 10),
      limits = c(0, NA)
      ) +
    scale_color_gradient(
      low = "red", 
      high = "blue"
      ) +
    facet_wrap(
      facet = ~ facet,
      ncol = n_col
      ) + 
    theme(
      strip.text.x     = element_text(),
      panel.background = element_rect(fill = "white"),
      plot.title       = element_text(hjust = 0.5)
    )
  
  ggsave(paste(id_plot, "_WC.jpeg", sep = ""), path = "Output/Plots/tf_idf", width = 16, height = 8)

}
```

## Course Level
The function `prepare_tf_idf_course` selects 25 random courses, the top ten words with highest tf_idf, and finally renames a facet to 'Course ID').
We apply the function to `id_plot` and pipe it to produce graphs.
```{r tf-idf course, cache = TRUE}
prepare_tf_idf_course <- function(data){
  
  data %>%
    
    # Selection 25 courses randomly
    filter(
      `Course ID` %in% sample(
        x = unique(.$`Course ID`),
        size = 25, 
        replace = FALSE
        )
      ) %>%
    
    # Select top 10 words per course 
    group_by(
      `Course ID`
      ) %>%
    filter(
      n >= 2
      ) %>%
    top_n(
      n = 10,
      wt = tf_idf
      ) %>%
    ungroup %>%
    
    # Prepare data for function `plot_tf_idf()`
    rename(
      facet = `Course ID`
      )

  }


set.seed(123)
tf_idf$overview %>%
  prepare_tf_idf_course %>%
  plot_tf_idf(id_plot = "Course_overview")

set.seed(123)
tf_idf$manual %>%
  prepare_tf_idf_course %>%
  plot_tf_idf(id_plot = "Course_manual")

rm(prepare_tf_idf_course)
```

## Cluster Level
```{r tf-idf cluster, cache = TRUE}
prepare_tf_idf_cluster <- function(data){
  
  data %>%
    
    # Include the variable Cluster
    left_join(
      select(d_course, `Course ID`, Cluster),
      by = "Course ID"
      ) %>%
    filter(
      !is.na(Cluster)
      ) %>%
    
    # Select top 10 words per cluster
    group_by(
      Cluster, 
      word
      ) %>%
    summarise(
      tf_idf = sum(tf_idf),
      n = sum(n)
      ) %>%
    filter(
      n >= 10
      ) %>%
    top_n(
      n = 10, 
      wt = tf_idf
      ) %>%
    ungroup %>%
    
    # Prepare data for function `plot_tf_idf()`
    rename(
      facet = Cluster
      )
  
}

tf_idf$overview %>%
  prepare_tf_idf_cluster %>%
  plot_tf_idf(id_plot = "Cluster_overview")

tf_idf$manual %>%
  prepare_tf_idf_cluster %>%
  plot_tf_idf(id_plot = "Cluster_manual")

rm(prepare_tf_idf_cluster)
```

## Concentration Level
```{r tf-idf concentration, cache = TRUE}
prepare_tf_idf_concentration <- function(data){
  
  data %>%
    
    # Include the variable Concentration
    left_join(
      select(d_course, `Course ID`, Concentration, `Concentration (additional)`),
      by = "Course ID"
      ) %>%
    filter(
      !is.na(Concentration)
      ) %>%
    gather(
      key = X,
      value = Concentration,
      Concentration, `Concentration (additional)`,
      na.rm = TRUE
      ) %>%
    
    # Select top 10 words per cluster
    group_by(
      Concentration, 
      word
      ) %>%
    summarise(
      tf_idf = sum(tf_idf),
      n = sum(n)
      ) %>%
    filter(
      n >= 10
      ) %>%
    top_n(
      n = 10,
      wt = tf_idf
      ) %>%
    ungroup %>%
    
    # Prepare data for function `plot_tf_idf()`
    rename(
      facet = Concentration
      )
  
} 

tf_idf$overview %>%
  prepare_tf_idf_concentration %>%
  plot_tf_idf(id_plot = "Concentration_overview")

tf_idf$manual %>%
  prepare_tf_idf_concentration %>%
  plot_tf_idf(id_plot = "Concentration_manual")

rm(prepare_tf_idf_concentration, plot_tf_idf)
```

# LDA
##Setup
###Cast DTM
The function `my_cast_dtm` casts our dataframes (catalogues and manuals) into a DTM.
```{r my_cast_dtm, eval = TRUE}
#Function
my_cast_dtm <- function(data) data %>%
  count(`Course ID`, word) %>%
  cast_dtm(`Course ID`, word, n)

#Application
d_cast <- d_text %>% map(my_cast_dtm)

#Clean workspace
rm(my_cast_dtm)
```
###Controls
The control parameters we will use to train our LDA model are:
```{r controls, eval = TRUE}
my_control <- list(
  
  nstart = 20,
  seed   = 1 : 20, #must have length nstart
  best   = TRUE,
  
  burnin = 1000,
  iter   = 6000,
  thin   = 100
  
)
```

We create a helping control that we can use when we need fast convergence for testing new code. Thus, the control parameters we use for fast testing are:
```{r fast control, eval = TRUE}
my_control_fast <- list(
  
  nstart = 1,
  seed   = 1,
  best   = TRUE,
  
  burnin = 5,
  iter   = 10,
  thin   = 2
  
)
```

## Fitting Model
The function `my_LDA` fits an LDA model on the DTM using the previous controls.
```{r my_LDA, eval= TRUE}
my_LDA <- function(n_topics, corpus, control = my_control){

  LDA_model <- LDA(
    x       = corpus,
    k       = n_topics,
    method  = "Gibbs",
    control = control
    )
}

```

###Extracting functions
The following functions are used as input in a map to compute and evaluate topic models. 
```{r extracting functions, eval = TRUE}
#extract dtm
extract_dtm <- function(origin) d_cast[[origin]]

#extract Distributions (Gamma/Beta)
get_distributions <- function(model, distrib_str){ #input gamma or beta
  tidy(model, matrix = distrib_str)        %>%
    mutate(topic = paste("Topic", topic) ) %>%
      arrange(topic, desc(!!sym(distrib_str)))
}

#perplexity Function
my_perplexity <- function(model, dtm){
  perplexity(model, dtm, estimate_theta = FALSE)
}

```

The following functions allow us to inspect the results from our models(inspect distributions and get kw):
```{r inspection functions, eval = TRUE}

get_top_terms_topic <- function(d_beta){
  data.frame(d_beta) %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    select(topic, term) %>%
    group_by(topic) %>%
    summarise(top_words = list(term))
  
}

get_top_topics_course <- function(d_gamma){
  data.frame(d_gamma) %>%
    group_by(document) %>%
    top_n(5, gamma) %>%
    select(document, topic) %>%
    summarise(top_topic = list(topic))
}

get_kw <- function(d_beta){
  data.frame(d_beta) %>%
    group_by(topic) %>%
    top_n(3, beta) %>%
    ungroup()%>%
    pull(term)%>%
    unique()
}

```

###Training Models
The function `train_topic_model` takes the type of text input ("catalogues", "manuals", c("catalogues", "manuals")) and number of topics to be trained. It fits an LDA model on the data and saves the output as a tibble where 
each row corresponds to a number of topics per text input (e.g. catalogues_50). It saves the DTM used to build the model, the corpus of origin (overview or manuals), the word and topic distributions results from that model, it's evaluation measures, and vectors of words and topics that allow us humans to get a feel of the model, along with some key words that will be used for the app.

```{r function train_model, eval = TRUE}
train_topic_model <- function(which_text, topic_numbers){
  topic_model <- tibble( 
    n_topic = rep(topic_numbers, each = length(d_text)) 
    ) %>%
  
    mutate(Origin     = rep(c("catalogues", "manuals"), length(topic_numbers)), 
           `Topic ID` = paste(Origin, n_topic, sep = "_"),
         
           #Document Term Matrix
           DTM        = pmap(.l = list(Origin), .f = extract_dtm)) %>%
  
  filter(Origin %in% which_texts) %>% ######################## was previously == instead of %in%
  
  mutate(
         #Fit Model
         Model  = pmap(.l = list(n_topic, DTM), .f = my_LDA, control = my_control), #my_control
         
         #Gamma distribution
         Gamma = pmap(.l = list(Model), .f = get_distributions, distrib_str = "gamma"),

         #Beta distribution
         Beta = pmap(.l = list(Model),  .f = get_distributions, distrib_str = "beta"),

         #Perplexity
         Perplexity = pmap_dbl(.l = list(Model, DTM), .f = my_perplexity),

         #Loglikelihood
         LogLikelihood = pmap_dbl(.l = list(Model), .f = logLik), #this removes the degrees of freedom data


         #
         ##Inspect

         #top words per topic
         top_terms_topic = map(Beta, .f = get_top_terms_topic),

         #top topics per course
         top_topic_course = map(Gamma, .f = get_top_topics_course),

         #Select keywords
         kw = map(Beta, .f = get_kw)
         )
}

```

We apply the function separately and take the time it takes to fit the model.

Note: The manuals take a long time to run, we keep the trainings of catalogues and manuals separate so that changes in one do not require retraining the other. 
####Catalogues
```{r LDA fit catalogues, cache = TRUE, eval = TRUE}
#
## Catalogues

#Input vectors
which_texts <- "catalogues"
topic_numbers <-seq(from = 5, to = 200, by = 5)

#Intitial time
initial_time <- Sys.time()

#Application to catalogues
catalogues_topic_model <- train_topic_model(which_texts, topic_numbers) 

#Final Time
final_time <- Sys.time()
took_how_long <- final_time - initial_time
print(paste(which_texts, took_how_long, sep =" "))

#Save
save(catalogues_topic_model, file = paste("./Output/", which_texts, "_topic_model_tibble.RDATA", sep = ""))
```

####Manuals
```{r LDA fit manuals, cache = TRUE, evel = TRUE }
#
## Manuals

#Input vectors
which_texts <- "manuals"

#Intitial time
initial_time <- Sys.time()

#Application to catalogues
manuals_topic_model <- train_topic_model(which_texts, topic_numbers)  #number of topics stayed the same cause we want to merge them. 

#Final Time
final_time <- Sys.time()
took_how_long <- final_time - initial_time
print(paste(which_texts, took_how_long, sep =" "))

#Save
save(manuals_topic_model, file = paste("./Output/", which_texts, "_topic_model_tibble.RDATA", sep = ""))
```

```{r clean workspace functions, eval = TRUE}
# Clean workspace
# rm(extract_dtm, get_distributions, my_perplexity,
#    get_top_terms_topic, get_top_topics_course, get_kw)
```

If we trained separately and wish to merge into a single tibble: 
```{r merge if trained separately, eval = TRUE}
tb_topic_model <- tibble()

if(exists("manuals_topic_model") && exists("catalogues_topic_model")){
  tb_topic_model <- rbind(catalogues_topic_model, manuals_topic_model)
} else  if(exists("catalogues_topic_model")){
  tb_topic_model <- catalogues_topic_model 
  } else if (exists("manuals_topic_model")){
    tb_topic_model <- catalogues_topic_model
} 

save(tb_topic_model, file = "./Output/full_topic_model_tibble.RDATA")
```

##Model Evaluation CHANGE TO CONFERENCE CODE
###Plots
To evaluate how many topics are optiomal, we graph the perplexity and log Likelihood of models against the number of topics.
```{r perplexity plots, eval = TRUE}
#Function
plot_evaluation <- function(origin, data = tb_topic_model){
  
  #data
  data %>%
    
    #Wrangling
    select(n_topic, Origin, Perplexity, LogLikelihood) %>%
    gather(key = "Eval Type", value = "Eval Value", c(Perplexity, LogLikelihood)) %>%
    filter(Origin == origin) %>%
    
    #Graphics
    ggplot(mapping = aes(x = n_topic, y = `Eval Value`)) +
    geom_point() +
    geom_line() +
    facet_grid(rows = vars(`Eval Type`), scales = "free") +
    theme_minimal()
    
}

#Plot
plot_evaluation("catalogues")
plot_evaluation("manuals")
```

###Value
```{r best number of topics extraction}
n_topics_best_manuals    <- manuals_topic_model    %>% top_n(1, LogLikelihood) %>% pull(n_topic)
n_topics_best_catalogues <- catalogues_topic_model %>% top_n(1, LogLikelihood) %>% pull(n_topic)
```

##Best model
We extract the best model and use it to innitialise the the second run of LDA algorithm with more burns and iterations
###catalogues best model
```{r best catalogues tm, eval = T}
catalogues_tm <- tb_topic_model %>% filter(Origin == "catalogues")

catalogue_dtm <- catalogues_tm$DTM[[1]]

catalogue_lda <- catalogues_tm %>% filter(n_topic == n_topics_best_catalogues) %>% pull(Model) %>%.[[1]]

#Train best model 
catalogues_best_lda <- LDA(x = catalogue_dtm, 
                model = catalogue_lda, 
                control =  list(
                  nstart = 50,
                  seed   = 1 : 50, #must have length nstart
                  best   = TRUE,
                  burnin = 100,
                  iter   = 10000,
                  thin   = 2000) 
                )

save(catalogues_best_lda, file = "./Output/catalogues_best_model.RDATA") 
```
###manuals best model
NOTE: BEST MANUALS NUMBER OF TOPICS TO BE DETERMINED 
```{r best manuals tp, eval = F}
manuals_tm    <- tb_topic_model %>% filter(Origin == "manuals")

manual_dtm <- manuals_tm$DTM[[1]]

manual_lda <- manuals_tm %>% filter(n_topic == n_topics_best_manuals) %>% pull(Model) %>%.[[1]]

#Train best model
manuals_best_lda <- LDA(x = manual_dtm, 
                model = manual_lda, 
                control =  list(
                  nstart = 50,
                  seed   = 1 : 50, #must have length nstart
                  best   = TRUE,
                  burnin = 100,
                  iter   = 10000,
                  thin   = 2000) 
                )

save(manuals_best_lda, file = "./Output/manuals_best_model.RDATA") 
```

##Best model pass to app
```{r load best models, eval = F}
load("./Output/full_topic_model_tibble.RDATA") #contains both manuals and catalogues

#Alternatively:
#load("./Output/catalogues_topic_model_tibble.RDATA")

#Best models:
load("./Output/catalogues_best_model.RDATA")
load("./Output/manuals_best_model.RDATA")
```

```{r best models clean for app, eval = TRUE}
#helper function
get_best_lda <- function(Origin){
  if(Origin == "catalogues"){
    
    return (catalogues_best_lda[[1]])
    
  } else if(Origin == "manuals"){
    
      return(manuals_best_lda[[1]])
    }
}

best_model <- tibble( 
  n_topic = c(n_best_catalogues, n_best_manuals)
  ) %>%
  
  mutate(Origin     = c("catalogues", "manuals"),
         `Topic ID` = paste(Origin, n_topic, sep = "_"),
         
         #Document Term Matrix
         DTM        = pmap(.l = list(Origin), .f = extract_dtm),
  
         #Fit Model
         Model  = pmap(.l = list(Origin), .f = get_best_lda),
         
         #Gamma distribution
         Gamma = pmap(.l = list(Model), .f = get_distributions, distrib_str = "gamma"),

         #Beta distribution
         Beta = pmap(.l = list(Model),  .f = get_distributions, distrib_str = "beta"),

         #Perplexity
         Perplexity = pmap_dbl(.l = list(Model, DTM), .f = my_perplexity),

         #Loglikelihood
         LogLikelihood = pmap_dbl(.l = list(Model), .f = logLik), #this removes the degrees of freedom data

         #
         ##Inspect

         #top words per topic
         top_terms_topic = map(Beta, .f = get_top_terms_topic),

         #top topics per course
         top_topic_course = map(Gamma, .f = get_top_topics_course),

         #Select keywords
         kw = map(Beta, .f = get_kw)
         )


```

###Save best model
```{r save best model, eval = TRUE}
save(best_model, file = "./Output/best_model.RDATA")
```

#App
For our recommender system, we pass the distributions, number of topics, origin, and key words to the app.
##Distributions and Key words
The function `clean_kw` removes stopwords from our key words.
```{r function clean key words, eval = TRUE}
clean_kw <- function(kw_list){

  #stopwords
  kw_stopwords <- c("source", "primary", "type", "assess", "involve", "e.g", "discussion", "institute", "ha", "introduce",
                 "role","address", "student", "examination","coordinator", "maastrichtuniversity.nl", "identify", "manner", "mechanism", "concept", "hour",
                 "teach",  "depth","question", "specific", "busy","academic", "semester", "week", "test", "session", "team", "communicate", "theory", "text",
                 "library", "article", "examine", "current", "routledge", "temporary", "flow", "level", "study", "task")
  
  #remove stopwords
  kw <- setdiff(kw_list, kw_stopwords)
  #return
  return(kw[!is.na(kw)])
  
}

```
On our `topic_model` tibble we filter only information relevant for the app, and apply `clean_kw` on our `kw` column
```{r crop and clean TM for App, eval = TRUE }
app_topic_model <- best_model %>% select(`Topic ID`, n_topic, Origin, Gamma, Beta, kw) %>%
  mutate(kw = map(kw, .f = clean_kw))
```

##Convenience Vectors
For the app we also need to have a list of all courses and an example of a prospective semester
```{r for convenience, eval = TRUE}
# set seed
set.seed(1)

#
## All courses
course_all <- d_course %>%
  inner_join(
    d_transcript,
    by = "Course ID"
  ) %>%
  distinct(
    `Course ID`
  ) %>%
  # remove semester abroad, skills and projects
  filter(
    ! str_detect(`Course ID`, pattern = "SA|SKI|PRO")
  ) %>%
  pull

#
##courses following_semester
course_following_semester <- sample(x = course_all,
                                    size    = 60,
                                    replace = FALSE) %>% sort()

```

```{r adding convenience vectors, eval = TRUE}
app_model <- app_topic_model %>%
  mutate(`All Courses`     = list(course_all),
         `Sample semester` = list(course_following_semester),
         `Course Titles`   = list(course_titles) )


#rm(course_all, course_following_semester)
```
## Saving files for APP
```{r app files, eval = TRUE}
save(app_model, file = "App/Recommender System/app_model.RDATA")
save(app_model, file = "./Output/app_model.RDATA")
```